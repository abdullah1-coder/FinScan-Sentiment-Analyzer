{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVDqeqDbdNFT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv('/content/analyst_ratings_processed.csv')\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "pKkUjo1Jeu7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "7haX-CVukzBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['date']=pd.to_datetime(dataset['date'],errors='coerce')"
      ],
      "metadata": {
        "id": "G4aMRtgOgNXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "t01ZbKGgkyQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "TjtSep5llNCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "0_qmFrMPnA0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['title']=dataset['title'].str.lower()"
      ],
      "metadata": {
        "id": "LS9gICvBlW9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Special Characters"
      ],
      "metadata": {
        "id": "qKxE7oxcbDih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['title_clean'] = dataset['title'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "dataset['title_clean'] = dataset['title_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
        "dataset['title_clean'] = dataset['title_clean'].str.strip()"
      ],
      "metadata": {
        "id": "io9FPG-alnU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "g38-DpJnbMal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['tokens'] = dataset['title_clean'].str.split()"
      ],
      "metadata": {
        "id": "UUIHbJu4pjQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stop words\n"
      ],
      "metadata": {
        "id": "vb4QVYNUeWEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_stopwords = {\n",
        "    \"is\", \"am\", \"are\", \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"if\",\n",
        "    \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"into\", \"through\",\n",
        "    \"during\", \"before\", \"after\", \"to\", \"in\", \"on\", \"from\",\"this\",\"that\"\n",
        "}\n",
        "def filter_stopwords(word_list):\n",
        "    cleaned_list = []\n",
        "    for word in word_list:\n",
        "        if word.lower() not in my_stopwords:\n",
        "            cleaned_list.append(word)\n",
        "    return cleaned_list\n",
        "\n",
        "dataset['tokens_clean'] = dataset['tokens'].apply(filter_stopwords)"
      ],
      "metadata": {
        "id": "W0ZimfMLcCnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizer"
      ],
      "metadata": {
        "id": "EGcGvlPAjmV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def apply_lemmatization(token_list):\n",
        "    return [lemmatizer.lemmatize(word) for word in token_list]\n",
        "\n",
        "dataset['lemmatized_tokens'] = dataset['tokens_clean'].apply(apply_lemmatization)"
      ],
      "metadata": {
        "id": "knW_nfyUgY-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization\n"
      ],
      "metadata": {
        "id": "C5mW9A24nbzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['final_text'] = dataset['lemmatized_tokens'].apply(lambda x: \" \".join(x))"
      ],
      "metadata": {
        "id": "8BtYE-uMlady"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=20000)\n",
        "X = tfidf.fit_transform(dataset['final_text'])\n"
      ],
      "metadata": {
        "id": "9bEKT0cdniJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labeling"
      ],
      "metadata": {
        "id": "8mdFf9aUrTCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "vader = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "krP5kRgcnxp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "new_words = {\n",
        "    'lower': -2.5,\n",
        "    'higher': 2.5,\n",
        "    'rise': 1.5,\n",
        "    'fall': -1.5,\n",
        "    'drop': -2.0,\n",
        "    'surge': 2.0,\n",
        "    'crash': -3.5,\n",
        "    'growth': 2.0,\n",
        "    'loss': -2.5,\n",
        "    'profit': 2.5\n",
        "}\n",
        "vader.lexicon.update(new_words)\n",
        "def get_sentiment_label_v2(text):\n",
        "\n",
        "    if 'lower' in text or 'down' in text or 'fall' in text:\n",
        "        return -1\n",
        "\n",
        "    if 'higher' in text or 'rise' in text or 'surge' in text:\n",
        "        return 1\n",
        "\n",
        "\n",
        "    score = vader.polarity_scores(text)['compound']\n",
        "    if score >= 0.3:\n",
        "        return 1\n",
        "    elif score <= -0.3:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "dataset['sentiment_label'] = dataset['final_text'].apply(get_sentiment_label_v2)\n",
        "\n",
        "\n",
        "dataset_filtered = dataset[dataset['sentiment_label'] != 0].copy()\n",
        "y=dataset['sentiment_label']"
      ],
      "metadata": {
        "id": "6PKy23nLrY_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset['sentiment_label'] = dataset['final_text'].apply(get_sentiment_label_v2)\n",
        "\n",
        "dataset = dataset[dataset['sentiment_label'] != 0]\n",
        "print(dataset['sentiment_label'].value_counts())"
      ],
      "metadata": {
        "id": "fFld6ePUrkX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_balanced, y_balanced = rus.fit_resample(X, y)\n",
        "\n",
        "\n",
        "print(\"New Balanced Counts:\")\n",
        "print(y_balanced.value_counts())"
      ],
      "metadata": {
        "id": "tA_hKzmbwkZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wh2Asxy1wx0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "douEC6edw3GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test,y_test)*100"
      ],
      "metadata": {
        "id": "ApU9xQdmPxJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
      ],
      "metadata": {
        "id": "ODehMUhgyApw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = f1_score(y_test, model.predict(X_test), average='weighted') * 100\n",
        "f1"
      ],
      "metadata": {
        "id": "ebDEHu6ry1eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(y_test, model.predict(X_test), average='weighted')*100"
      ],
      "metadata": {
        "id": "CG_nzYyuy5TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y_test, model.predict(X_test), average='weighted')*100"
      ],
      "metadata": {
        "id": "FWZv0QwFy9l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test,model.predict(X_test))*100"
      ],
      "metadata": {
        "id": "Kyd3CEFwzBIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[-1, 1])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative (-1)', 'Positive (1)'],\n",
        "            yticklabels=['Negative (-1)', 'Positive (1)'])\n",
        "\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix: Financial Sentiment Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bgmKkSGI1SHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "text_input = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Type a financial headline here...',\n",
        "    description='Headline:',\n",
        "    layout={'width': '500px', 'height': '100px'}\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='Analyze Sentiment',\n",
        "    button_style='success',\n",
        "    tooltip='Click to see confidence'\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        user_text = text_input.value\n",
        "\n",
        "        if not user_text.strip():\n",
        "            print(\"Please enter a headline to analyze.\")\n",
        "            return\n",
        "\n",
        "\n",
        "        test_vector = tfidf.transform([user_text])\n",
        "\n",
        "\n",
        "        prediction = model.predict(test_vector)[0]\n",
        "        probabilities = model.predict_proba(test_vector)[0]\n",
        "\n",
        "\n",
        "        conf_index = 1 if prediction == 1 else 0\n",
        "        confidence = probabilities[conf_index] * 100\n",
        "\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        if prediction == 1:\n",
        "            print(f\"RESULT: POSITIVE (1) ðŸš€\")\n",
        "            print(f\"CONFIDENCE: {confidence:.2f}%\")\n",
        "        else:\n",
        "            print(f\"RESULT: NEGATIVE (-1) ðŸ“‰\")\n",
        "            print(f\"CONFIDENCE: {confidence:.2f}%\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(text_input, button, output)"
      ],
      "metadata": {
        "id": "RhNeoVZQR5Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICb4iqyZTVWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}